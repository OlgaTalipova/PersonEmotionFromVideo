# Determining a person's emotion from video

Цель проекта - определить по видеофрагменту тип эмоции, которую испытывает человек. Задача сводится к классификации следующих эмоциональных состояний:

0. гневное
1. отвращение
2. страх
3. счастливое
4. нейтральное
5. печальное
6. удивление

В качестве базы исследования был взят [OMG Emotion Dataset](https://github.com/knowledgetechnologyuhh/OMGEmotionChallenge) - набор данных эмоций за одну минуту.  
Датасет состоит из ссылок на 420 примеров видео с эмоциями средней продолжительностью 1 минута, собранных с различных каналов Youtube. Каждое видео поделено на фрагменты, соответствующие определенной эмоции.  
Из каждого фрагмента я выделила блоки по 20 кадров. Для малочисленных классов (1,2,6) из исходного файла выделяла несколько блоков - по 20 через 10 кадров для увеличения числа примеров.  
В каждом кадре из полученного блока вырезала изображение лица, используя библиотеку dlib, переводила в черно-белое, делила на 255 и сохраняла в размере 80х80, в итоге получила датасет из массивов размерностью 20х80х80

На следующем этапе были произведены анализ, очистка, дополнение и балансировка исходного датасета:

- удалены фрагменты видео, которые были отнесены к нескольким классам одновременно.
- исправлены ошибочные назначения классов для фрагментов видео.
- численность фрагментов в каждом классе сбалансирована - часть видео из наиболее многочисленных классов удалена из датасета, классы с наименьшим количеством видео были дополнены фрагментами, скачанными из Youtube.

Ссылка на [сбалансированные классы видеофрагментов](https://drive.google.com/drive/folders/1-1kgXbbgC1i76vukIWE0VJcHM-ipEc_9?usp=sharing)

Исправленный датасет был переведен в npy-массивы для подачи на вход нейросети.

Ссылка на [итоговый датасет](https://drive.google.com/drive/folders/1rGTNQCP8QWcGa5bJ6H8mGJH7LlKiN-M3?usp=share_link)

В приведенных блокнотах юпитера содержатся эксперименты с различными архитектурами нейронных сетей.

Наилучшие показатели точности на валидационной выборке (84%) в результате обучения получены моделью сверточной сети со следующей архитектурой:
![image](https://github.com/OlgaTalipova/PersonEmotionFromVideo/assets/95358146/67fd13b3-4091-4d91-ae77-99f527143550)

Примеры распознавания эмоции на видео:
![image](https://github.com/OlgaTalipova/PersonEmotionFromVideo/assets/95358146/29fbe538-ca51-4172-914f-f68c58ed63bb)

Обученная модель была развернута на сервисе Anvil. Для просмотра работы модели необходимо запустить все ячейки ноутбука predict.ipynb из каталога [my_app](https://drive.google.com/drive/folders/1OdYtzFMMqFe1aqC8rtblrpeDkXpFbIzU?usp=share_link) для запуска серверной части сервиса, а затем перейти по [ссылке](https://emotion.anvil.app/)

![image](https://github.com/OlgaTalipova/PersonEmotionFromVideo/assets/95358146/c51f585f-1eb3-49f8-8ceb-45cb2ecb26e7)

